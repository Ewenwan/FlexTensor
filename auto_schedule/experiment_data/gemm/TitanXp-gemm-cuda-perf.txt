full search:
gemm_gemm_(32, 32, 32)_cuda(0) use 0.0035954999999999997 ms

gemm_gemm_(64, 64, 64)_cuda(0) use 0.0042938 ms

gemm_gemm_(128, 128, 128)_cuda(0) use 0.007368800000000001 ms

gemm_gemm_(256, 256, 256)_cuda(0) use 0.0168454 ms

gemm_gemm_(512, 512, 512)_cuda(0) use 0.0673134 ms

gemm_gemm_(1024, 1024, 1024)_cuda(0) use 0.4209670000000001 ms

gemm_gemm_(2048, 2048, 2048)_cuda(0) use 3.150483 ms


q-search:
gemm_gemm_(32, 32, 32)_cuda(0) use 0.0029702 ms

gemm_gemm_(64, 64, 64)_cuda(0) use 0.0034637 ms

gemm_gemm_(128, 128, 128)_cuda(0) use 0.0076194999999999995 ms

gemm_gemm_(256, 256, 256)_cuda(0) use 0.0172477 ms

gemm_gemm_(512, 512, 512)_cuda(0) use 0.07392339999999999 ms

gemm_gemm_(1024, 1024, 1024)_cuda(0) use 0.40571 ms

gemm_gemm_(2048, 2048, 2048)_cuda(0) use 3.2193722 ms


pytorch only:
pytorch baselines gemm for target cuda (0):
layer 0
Use 0.010109(ms)
layer 1
Use 0.008205(ms)
layer 2
Use 0.007987(ms)
layer 3
Use 0.010125(ms)
layer 4
Use 0.062259(ms)
layer 5
Use 0.287437(ms)
layer 6
Use 1.943245(ms)
Done!


pytorch + cudnn:
pytorch baselines gemm for target cuda (0):
layer 0
Use 0.009421(ms)
layer 1
Use 0.008192(ms)
layer 2
Use 0.008115(ms)
layer 3
Use 0.010122(ms)
layer 4
Use 0.062566(ms)
layer 5
Use 0.286822(ms)
layer 6
Use 1.943757(ms)
Done!

full-search:
conv2d_dilation_(1, 3, 448, 448, 64, 7, 2, 3, 2, 1)_cuda(0) use 0.3708917 ms

conv2d_dilation_(1, 64, 112, 112, 192, 3, 1, 1, 2, 1)_cuda(0) use 0.6316377 ms

conv2d_dilation_(1, 192, 56, 56, 128, 1, 1, 0, 2, 1)_cuda(0) use 0.041484099999999996 ms

conv2d_dilation_(1, 128, 56, 56, 256, 3, 1, 1, 2, 1)_cuda(0) use 0.3400028 ms


q-search:
conv2d_dilation_(1, 3, 448, 448, 64, 7, 2, 3, 2, 1)_cuda(0) use 0.3301578 ms

conv2d_dilation_(1, 64, 112, 112, 192, 3, 1, 1, 2, 1)_cuda(0) use 0.8685170000000001 ms

conv2d_dilation_(1, 192, 56, 56, 128, 1, 1, 0, 2, 1)_cuda(0) use 0.0422926 ms

conv2d_dilation_(1, 128, 56, 56, 256, 3, 1, 1, 2, 1)_cuda(0) use 0.3514744 ms


pytorch:
pytorch baselines dilation convolution for target cuda (0):
layer 0
Use 0.492986(ms)
layer 1
Use 0.628989(ms)
layer 2
Use 0.124208(ms)
layer 3
Use 0.362659(ms)
Done!


pytorch + cudnn:
pytorch baselines dilation convolution for target cuda (0):
layer 0
Use 9.316202(ms)
layer 1
Use 11.045242(ms)
layer 2
Use 1.060749(ms)
layer 3
Use 6.285555(ms)
Done!



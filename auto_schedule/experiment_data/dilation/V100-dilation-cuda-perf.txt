full search:
conv2d_dilation_(1, 256, 56, 56, 256, 1, 1, 0, 2, 1)_cuda(3) use 0.0630213 ms

conv2d_dilation_(1, 256, 56, 56, 512, 3, 1, 1, 2, 1)_cuda(3) use 0.9324899 ms

conv2d_dilation_(1, 512, 28, 28, 256, 1, 1, 0, 2, 1)_cuda(3) use 0.0488571 ms

conv2d_dilation_(1, 256, 28, 28, 512, 3, 1, 1, 2, 1)_cuda(3) use 0.3120014 ms

conv2d_dilation_(1, 512, 28, 28, 512, 1, 1, 0, 2, 1)_cuda(3) use 0.0875735 ms

conv2d_dilation_(1, 512, 28, 28, 1024, 3, 1, 1, 2, 1)_cuda(3) use 1.0808896000000001 ms

conv2d_dilation_(1, 1024, 14, 14, 512, 1, 1, 0, 2, 1)_cuda(3) use 0.06444839999999999 ms

conv2d_dilation_(1, 512, 14, 14, 1024, 3, 1, 1, 2, 1)_cuda(3) use 0.275729 ms

conv2d_dilation_(1, 1024, 14, 14, 1024, 3, 1, 1, 2, 1)_cuda(3) use 0.7484357 ms

conv2d_dilation_(1, 1024, 14, 14, 1024, 3, 2, 1, 2, 1)_cuda(3) use 0.3857237 ms

conv2d_dilation_(1, 1024, 7, 7, 1024, 3, 1, 1, 2, 1)_cuda(3) use 0.2557025 ms


pytorch only:
pytorch baselines dilation convolution for target cuda (0):
layer 0
Use 0.227434(ms)
layer 1
Use 0.829533(ms)
layer 2
Use 0.131702(ms)
layer 3
Use 0.287549(ms)
layer 4
Use 0.130342(ms)
layer 5
Use 0.731216(ms)
layer 6
Use 0.143283(ms)
layer 7
Use 0.257411(ms)
layer 8
Use 0.415683(ms)
layer 9
Use 0.314819(ms)
layer 10
Use 0.312685(ms)
Done!


pytorch + cudnn:
pytorch baselines dilation convolution for target cuda (0):
layer 0
Use 0.265968(ms)
layer 1
Use 0.995382(ms)
layer 2
Use 0.150298(ms)
layer 3
Use 0.461850(ms)
layer 4
Use 0.167309(ms)
layer 5
Use 1.133174(ms)
layer 6
Use 0.164832(ms)
layer 7
Use 0.703725(ms)
layer 8
Use 0.951891(ms)
layer 9
Use 0.828490(ms)
layer 10
Use 0.775114(ms)
Done!
